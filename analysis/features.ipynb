{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/schmidt/experiments-vldb2025/.venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting duckdb\n",
      "  Downloading duckdb-1.4.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (14 kB)\n",
      "Downloading duckdb-1.4.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (20.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.4/20.4 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: duckdb\n",
      "Successfully installed duckdb-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets\n",
    "%pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[07:00:10]\u001b[0m\u001b[2;36m \u001b[0m \u001b[1;33mINFO\u001b[0m        Extracting features for job version v0.\u001b[1;36m0\u001b[0m \u001b[1m(\u001b[0mSQLStorm:     \n",
      "\u001b[2;36m           \u001b[0m             \u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                                  \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m \u001b[1;33mINFO\u001b[0m        Extracting features for tpchSf1 version v0.\u001b[1;36m0\u001b[0m \u001b[1m(\u001b[0mSQLStorm: \n",
      "\u001b[2;36m           \u001b[0m             \u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                                  \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m \u001b[1;33mINFO\u001b[0m        Extracting features for tpcdsSf1 version v0.\u001b[1;36m0\u001b[0m           \n",
      "\u001b[2;36m           \u001b[0m             \u001b[1m(\u001b[0mSQLStorm: \u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                       \n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m \u001b[1;33mINFO\u001b[0m        Extracting features for stackoverflow_dba version v1.\u001b[1;36m0\u001b[0m  \n",
      "\u001b[2;36m           \u001b[0m             \u001b[1m(\u001b[0mSQLStorm: \u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                        \n",
      "\u001b[2;36m[07:00:54]\u001b[0m\u001b[2;36m \u001b[0m \u001b[1;33mINFO\u001b[0m        Extracting features for job version v1.\u001b[1;36m0\u001b[0m \u001b[1m(\u001b[0mSQLStorm:     \n",
      "\u001b[2;36m           \u001b[0m             \u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                   \n",
      "\u001b[2;36m[07:01:19]\u001b[0m\u001b[2;36m \u001b[0m \u001b[1;33mINFO\u001b[0m        Extracting features for tpchSf1 version v1.\u001b[1;36m0\u001b[0m \u001b[1m(\u001b[0mSQLStorm: \n",
      "\u001b[2;36m           \u001b[0m             \u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                                   \n",
      "\u001b[2;36m[07:01:55]\u001b[0m\u001b[2;36m \u001b[0m \u001b[1;33mINFO\u001b[0m        Extracting features for tpcdsSf1 version v1.\u001b[1;36m0\u001b[0m           \n",
      "\u001b[2;36m           \u001b[0m             \u001b[1m(\u001b[0mSQLStorm: \u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m                                        \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import features\n",
    "from log import log\n",
    "\n",
    "\n",
    "def extract_features(dataset: str = \"stackoverflow_dba\", version: str = \"v1.0\", sqlstorm: bool = True):\n",
    "    os.makedirs(f\"features/{version}\", exist_ok=True)\n",
    "    input_file = f\"../results/{version}/features_raw/{dataset}{'_sqlstorm_' + version if sqlstorm else ''}.csv\"\n",
    "    output_file = f\"features/{version}/{dataset}{'_sqlstorm' if sqlstorm else ''}.csv\"\n",
    "\n",
    "    log.info(f\"Extracting features for {dataset} version {version} (SQLStorm: {sqlstorm})\")\n",
    "    with log.file(output_file.replace(\".csv\", \".log\")) as log_file:\n",
    "        features.compute(input_file, output_file=output_file)\n",
    "\n",
    "\n",
    "extract_features(dataset=\"job\", version=\"v0.0\", sqlstorm=False)\n",
    "extract_features(dataset=\"tpchSf1\", version=\"v0.0\", sqlstorm=False)\n",
    "extract_features(dataset=\"tpcdsSf1\", version=\"v0.0\", sqlstorm=False)\n",
    "\n",
    "extract_features(dataset=\"stackoverflow_dba\", version=\"v1.0\", sqlstorm=True)\n",
    "extract_features(dataset=\"job\", version=\"v1.0\", sqlstorm=True)\n",
    "extract_features(dataset=\"tpchSf1\", version=\"v1.0\", sqlstorm=True)\n",
    "extract_features(dataset=\"tpcdsSf1\", version=\"v1.0\", sqlstorm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "\n",
    "def load_features(dataset: str = \"stackoverflow_dba\", version: str = \"v1.0\", sqlstorm: bool = True):\n",
    "    file = f\"features/{version}/{dataset}{'_sqlstorm' if sqlstorm else ''}.csv\"\n",
    "    features = duckdb.read_csv(file)\n",
    "    operators = duckdb.read_csv(file.replace(\".csv\", \"_operators.csv\"))\n",
    "    expressions = duckdb.read_csv(file.replace(\".csv\", \"_expressions.csv\"))\n",
    "\n",
    "    operators2 = duckdb.sql(\"select query, operator from operators where operator <> 'Result' and operator <> 'GroupJoin' and operator <> 'Map' and operator <> 'IterationScan' and operator <> 'InlineTable'\"\n",
    "                            \"union all select query, 'Join' as operator from operators where operator = 'GroupJoin'\"\n",
    "                            \"union all select query, 'GroupBy' as operator from operators where operator = 'GroupJoin'\")\n",
    "    expressions2 = duckdb.sql(\"select query, expression, category, type from expressions where category <> 'base'\")\n",
    "\n",
    "    return features, operators2, expressions2\n",
    "\n",
    "\n",
    "job_small, job_small_ops, job_small_exprs = load_features(dataset=\"job\", version=\"v0.0\", sqlstorm=False)\n",
    "tpch_small, tpch_small_ops, tpch_small_exprs = load_features(dataset=\"tpchSf1\", version=\"v0.0\", sqlstorm=False)\n",
    "tpcds_small, tpcds_small_ops, tpcds_small_exprs = load_features(dataset=\"tpcdsSf1\", version=\"v0.0\", sqlstorm=False)\n",
    "\n",
    "so, so_ops, so_exprs = load_features(dataset=\"stackoverflow_dba\", version=\"v1.0\", sqlstorm=True)\n",
    "job, job_ops, job_exprs = load_features(dataset=\"job\", version=\"v1.0\", sqlstorm=True)\n",
    "tpch, tpch_ops, tpch_exprs = load_features(dataset=\"tpchSf1\", version=\"v1.0\", sqlstorm=True)\n",
    "tpcds, tpcds_ops, tpcds_exprs = load_features(dataset=\"tpcdsSf1\", version=\"v1.0\", sqlstorm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────┬─────────────┬────────────────────┬────────────────────┬────────────────────┬────────────────────┐\n",
      "│ complexity  │ num_queries │  avg_querylength   │      avg_ops       │     avg_joins      │     avg_exprs      │\n",
      "│   varchar   │   double    │       double       │       double       │       double       │       double       │\n",
      "├─────────────┼─────────────┼────────────────────┼────────────────────┼────────────────────┼────────────────────┤\n",
      "│ high        │      3460.0 │ 1347.1583815028903 │  22.07687861271676 │  6.199421965317919 │  35.61387283236994 │\n",
      "│ low         │      4596.0 │  333.9264577893821 │  6.422758920800696 │ 1.8748912097476067 │  8.137946040034812 │\n",
      "│ medium      │     10195.0 │ 1168.0664051005394 │ 14.407552721922512 │  4.166552231486023 │ 29.123589995095635 │\n",
      "│ all         │     18251.0 │  991.9637828064216 │ 13.850747904224425 │ 3.9748506931127063 │ 25.069366062133582 │\n",
      "│ job_small   │       113.0 │  825.0442477876106 │ 17.292035398230087 │  7.646017699115045 │  18.61946902654867 │\n",
      "│ tpch_small  │        22.0 │  471.8181818181818 │  9.090909090909092 │ 2.8181818181818183 │ 12.863636363636363 │\n",
      "│ tpcds_small │       103.0 │ 1370.3300970873786 │ 19.271844660194176 │  6.300970873786408 │  37.51456310679612 │\n",
      "│ job         │     11714.0 │  885.6632234932559 │ 16.831312958852656 │ 5.7770189516817485 │ 14.981304422059075 │\n",
      "│ tpch        │     17036.0 │  875.9632542850435 │  16.04877905611646 │  5.207560460201925 │ 16.269135947405495 │\n",
      "│ tpcds       │     15242.0 │  981.8340112846083 │ 12.852578401784543 │  3.452171631019551 │ 18.032148012071907 │\n",
      "├─────────────┴─────────────┴────────────────────┴────────────────────┴────────────────────┴────────────────────┤\n",
      "│ 10 rows                                                                                             6 columns │\n",
      "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = duckdb.sql(\"\"\"\n",
    "           select   complexity, \n",
    "                    count(*)::double num_queries,\n",
    "                    avg(querylength) avg_querylength,\n",
    "                    avg((select count(*) from so_ops o where o.query = f.query))  as avg_ops,\n",
    "                    avg((select count(*) from so_ops e where e.query = f.query and operator = 'Join'))  as avg_joins,    \n",
    "                    avg((select count(*) from so_exprs e where e.query = f.query))  as avg_exprs,           \n",
    "           from so f\n",
    "           group by complexity\n",
    "           union all\n",
    "           select  'all' complexity, \n",
    "                    count(*)::double num_queries,\n",
    "                    avg(querylength) avg_querylength,\n",
    "                    avg((select count(*) from so_ops o where o.query = f.query))  as avg_ops,\n",
    "                    avg((select count(*) from so_ops e where e.query = f.query and operator = 'Join'))  as avg_joins,    \n",
    "                    avg((select count(*) from so_exprs e where e.query = f.query))  as avg_exprs,\n",
    "           from so f\n",
    "           union all\n",
    "           select 'job_small' complexity,\n",
    "                    count(*)::double num_queries,\n",
    "                    avg(querylength) avg_querylength,\n",
    "                    avg((select count(*) from job_small_ops o where o.query = f.query))  as avg_ops,\n",
    "                    avg((select count(*) from job_small_ops e where e.query = f.query and operator = 'Join'))  as avg_joins,    \n",
    "                    avg((select count(*) from job_small_exprs e where e.query = f.query))  as avg_exprs,\n",
    "           from job_small f\n",
    "           union all\n",
    "           select 'tpch_small' complexity,\n",
    "                    count(*)::double num_queries,\n",
    "                    avg(querylength) avg_querylength,\n",
    "                    avg((select count(*) from tpch_small_ops o where o.query = f.query))  as avg_ops,\n",
    "                    avg((select count(*) from tpch_small_ops e where e.query = f.query and operator = 'Join'))  as avg_joins,    \n",
    "                    avg((select count(*) from tpch_small_exprs e where e.query = f.query))  as avg_exprs,\n",
    "           from tpch_small f\n",
    "           union all\n",
    "           select 'tpcds_small' complexity,\n",
    "                    count(*)::double num_queries,\n",
    "                    avg(querylength) avg_querylength,\n",
    "                    avg((select count(*) from tpcds_small_ops o where o.query = f.query))  as avg_ops,\n",
    "                    avg((select count(*) from tpcds_small_ops e where e.query = f.query and operator = 'Join'))  as avg_joins,    \n",
    "                    avg((select count(*) from tpcds_small_exprs e where e.query = f.query))  as avg_exprs,\n",
    "            from tpcds_small f\n",
    "            union all\n",
    "            select 'job' complexity,\n",
    "                    count(*)::double num_queries,\n",
    "                    avg(querylength) avg_querylength,\n",
    "                    avg((select count(*) from job_ops o where o.query = f.query))  as avg_ops,\n",
    "                    avg((select count(*) from job_ops e where e.query = f.query and operator = 'Join'))  as avg_joins,    \n",
    "                    avg((select count(*) from job_exprs e where e.query = f.query))  as avg_exprs,\n",
    "            from job f\n",
    "            union all\n",
    "            select 'tpch' complexity,\n",
    "                    count(*)::double num_queries,\n",
    "                    avg(querylength) avg_querylength,\n",
    "                    avg((select count(*) from tpch_ops o where o.query = f.query))  as avg_ops,\n",
    "                    avg((select count(*) from tpch_ops e where e.query = f.query and operator = 'Join'))  as avg_joins,    \n",
    "                    avg((select count(*) from tpch_exprs e where e.query = f.query))  as avg_exprs,\n",
    "            from tpch f\n",
    "            union all\n",
    "            select 'tpcds' complexity,\n",
    "                    count(*)::double num_queries,\n",
    "                    avg(querylength) avg_querylength,\n",
    "                    avg((select count(*) from tpcds_ops o where o.query = f.query))  as avg_ops,\n",
    "                    avg((select count(*) from tpcds_ops e where e.query = f.query and operator = 'Join'))  as avg_joins,    \n",
    "                    avg((select count(*) from tpcds_exprs e where e.query = f.query))  as avg_exprs\n",
    "            from tpcds f\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────┬───────────────────────┬───────────────────────┬───────────────────────┬─────────────────────┬─────────────────────┐\n",
      "│   operator   │          low          │        medium         │         high          │        tpch         │        tpcds        │\n",
      "│   varchar    │        double         │        double         │        double         │       double        │       double        │\n",
      "├──────────────┼───────────────────────┼───────────────────────┼───────────────────────┼─────────────────────┼─────────────────────┤\n",
      "│ TableScan    │    2.8729329852045256 │     4.939676311917607 │     5.749132947976879 │  3.6818181818181817 │   7.223300970873787 │\n",
      "│ Join         │    1.8748912097476067 │     4.166552231486023 │     6.199421965317919 │  2.8181818181818183 │   6.300970873786408 │\n",
      "│ Sort         │    0.9873803307223673 │    1.0277587052476704 │    1.0965317919075144 │  0.8181818181818182 │  0.8349514563106796 │\n",
      "│ GroupBy      │    0.6818973020017406 │    2.3321235899950956 │    4.0098265895953755 │  1.3181818181818181 │  2.0776699029126213 │\n",
      "│ Select       │ 0.0039164490861618795 │    0.9720451201569397 │     1.783815028901734 │ 0.09090909090909091 │  1.1650485436893203 │\n",
      "│ Window       │                  NULL │    0.6392349190779794 │    1.0069364161849712 │                NULL │ 0.20388349514563106 │\n",
      "│ SetOperation │                  NULL │ 0.0036292300147130947 │  0.003179190751445087 │                NULL │  0.2815533980582524 │\n",
      "│ ArrayUnnest  │                  NULL │                  NULL │   0.48959537572254336 │                NULL │                NULL │\n",
      "│ Iteration    │                  NULL │                  NULL │   0.04566473988439306 │                NULL │                NULL │\n",
      "│ RegexSplit   │                  NULL │                  NULL │ 0.0011560693641618498 │                NULL │                NULL │\n",
      "├──────────────┴───────────────────────┴───────────────────────┴───────────────────────┴─────────────────────┴─────────────────────┤\n",
      "│ 10 rows                                                                                                                6 columns │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "TableScan & 2.87 & 4.94 & 5.75 & 3.68 & 7.22 \\\\\n",
      "Join & 1.87 & 4.17 & 6.20 & 2.82 & 6.30 \\\\\n",
      "Sort & 0.99 & 1.03 & 1.10 & 0.82 & 0.83 \\\\\n",
      "GroupBy & 0.68 & 2.33 & 4.01 & 1.32 & 2.08 \\\\\n",
      "Select & 0.004 & 0.97 & 1.78 & 0.09 & 1.17 \\\\\n",
      "Window & 0 & 0.64 & 1.01 & 0 & 0.20 \\\\\n",
      "SetOperation & 0 & 0.004 & 0.003 & 0 & 0.28 \\\\\n",
      "ArrayUnnest & 0 & 0 & 0.49 & 0 & 0 \\\\\n",
      "Iteration & 0 & 0 & 0.05 & 0 & 0 \\\\\n",
      "RegexSplit & 0 & 0 & 0.001 & 0 & 0 \\\\\n"
     ]
    }
   ],
   "source": [
    "result = duckdb.sql(\"\"\"\n",
    "                    with ops as (select distinct operator from so_ops),\n",
    "                         cops as (select complexity, operator, \n",
    "                                        count(*)::double / (select count(*) from so f2 where f2.complexity = f.complexity)::double as c \n",
    "                                    from so_ops o, so f where o.query = f.query group by complexity, operator),\n",
    "                         cops_tpch as (select operator, \n",
    "                                        count(*)::double / (select count(*) from tpch_small)::double as c \n",
    "                                    from tpch_small_ops o group by operator),\n",
    "                         cops_tpcds as (select operator, \n",
    "                                        count(*)::double / (select count(*) from tpcds_small)::double as c \n",
    "                                    from tpcds_small_ops o group by operator)\n",
    "                    select ops.operator, \n",
    "                        (select c from cops c where c.operator = ops.operator and c.complexity = 'low') as low,\n",
    "                        (select c from cops c where c.operator = ops.operator and c.complexity = 'medium') as medium,\n",
    "                        (select c from cops c where c.operator = ops.operator and c.complexity = 'high') as high,\n",
    "                        (select c from cops_tpch c where c.operator = ops.operator) as tpch,\n",
    "                        (select c from cops_tpcds c where c.operator = ops.operator) as tpcds\n",
    "                    from ops\n",
    "                    where ops.operator not in ('PipelineBreakerScan', 'Temp')\n",
    "                    order by low desc nulls last, medium desc nulls last, high desc nulls last\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "for r in result.fetchall():\n",
    "    def conv(c):\n",
    "        return f\"0\" if c is None else (f\"{c:.3f}\" if c < 0.01 else f\"{c:.2f}\")\n",
    "    print(f\"{r[0]} & {conv(r[1])} & {conv(r[2])} & {conv(r[3])} & {conv(r[4])} & {conv(r[5])} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────────┬──────────────────────┬──────────────────────┬────────────────────────┬─────────────────────┬──────────────────────┐\n",
      "│      category       │         low          │        medium        │          high          │        tpch         │        tpcds         │\n",
      "│       varchar       │        double        │        double        │         double         │       double        │        double        │\n",
      "├─────────────────────┼──────────────────────┼──────────────────────┼────────────────────────┼─────────────────────┼──────────────────────┤\n",
      "│ comparison_low      │    4.001087902523934 │   10.753310446297204 │     15.035260115606937 │   7.090909090909091 │    24.83495145631068 │\n",
      "│ agg_low             │     2.93668407310705 │    8.926630701324179 │      9.128901734104046 │   2.409090909090909 │     4.41747572815534 │\n",
      "│ cast                │   0.7071366405570061 │    2.758509073075037 │      2.538150289017341 │  0.7727272727272727 │   2.4563106796116503 │\n",
      "│ case                │  0.47976501305483027 │   2.3094654242275627 │      1.824277456647399 │ 0.18181818181818182 │   1.2718446601941749 │\n",
      "│ arithmetic_low      │ 0.013272410791993037 │  0.31574301128003923 │      0.577456647398844 │  1.7272727272727273 │   2.8058252427184467 │\n",
      "│ window_medium       │                 NULL │    2.248945561549779 │     3.3158959537572255 │                NULL │   0.7184466019417476 │\n",
      "│ nulls               │                 NULL │   1.4039234919077979 │     1.3167630057803468 │                NULL │  0.30097087378640774 │\n",
      "│ agg_medium          │                 NULL │  0.14350171652770966 │     0.3791907514450867 │                NULL │  0.13592233009708737 │\n",
      "│ string modification │                 NULL │  0.12525747915644925 │     0.7667630057803468 │ 0.22727272727272727 │  0.18446601941747573 │\n",
      "│ string matching     │                 NULL │  0.07366356056890633 │    0.17803468208092485 │  0.3181818181818182 │ 0.009708737864077669 │\n",
      "│ date                │                 NULL │ 0.028347229033840116 │   0.032947976878612714 │ 0.13636363636363635 │                 NULL │\n",
      "│ arithmetic_medium   │                 NULL │  0.01824423737126042 │   0.016473988439306357 │                NULL │   0.3786407766990291 │\n",
      "│ array               │                 NULL │ 0.018048062775870524 │      0.496242774566474 │                NULL │                 NULL │\n",
      "│ regex               │                 NULL │                 NULL │   0.004335260115606936 │                NULL │                 NULL │\n",
      "│ json                │                 NULL │                 NULL │   0.002890173410404624 │                NULL │                 NULL │\n",
      "│ comparison_medium   │                 NULL │                 NULL │ 0.00028901734104046245 │                NULL │                 NULL │\n",
      "├─────────────────────┴──────────────────────┴──────────────────────┴────────────────────────┴─────────────────────┴──────────────────────┤\n",
      "│ 16 rows                                                                                                                       6 columns │\n",
      "└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "comparison_low & 4.00 & 10.75 & 15.04 & 7.09 & 24.83 \\\\\n",
      "agg_low & 2.94 & 8.93 & 9.13 & 2.41 & 4.42 \\\\\n",
      "cast & 0.71 & 2.76 & 2.54 & 0.77 & 2.46 \\\\\n",
      "case & 0.48 & 2.31 & 1.82 & 0.18 & 1.27 \\\\\n",
      "arithmetic_low & 0.01 & 0.32 & 0.58 & 1.73 & 2.81 \\\\\n",
      "window_medium & 0 & 2.25 & 3.32 & 0 & 0.72 \\\\\n",
      "nulls & 0 & 1.40 & 1.32 & 0 & 0.30 \\\\\n",
      "agg_medium & 0 & 0.14 & 0.38 & 0 & 0.14 \\\\\n",
      "string modification & 0 & 0.13 & 0.77 & 0.23 & 0.18 \\\\\n",
      "string matching & 0 & 0.07 & 0.18 & 0.32 & 0.010 \\\\\n",
      "date & 0 & 0.03 & 0.03 & 0.14 & 0 \\\\\n",
      "arithmetic_medium & 0 & 0.02 & 0.02 & 0 & 0.38 \\\\\n",
      "array & 0 & 0.02 & 0.50 & 0 & 0 \\\\\n",
      "regex & 0 & 0 & 0.004 & 0 & 0 \\\\\n",
      "json & 0 & 0 & 0.003 & 0 & 0 \\\\\n",
      "comparison_medium & 0 & 0 & 0.000 & 0 & 0 \\\\\n"
     ]
    }
   ],
   "source": [
    "result = duckdb.sql(\"\"\"\n",
    "                    with exps as (select distinct category from so_exprs),\n",
    "                         cexps as (select complexity, category, count(*)::double / (select count(*) from so f2 where f2.complexity = f.complexity)::double as c from so_exprs o, so f where o.query = f.query group by complexity, category),\n",
    "                            cexps_tpch as (select category, count(*)::double / (select count(*) from tpch_small)::double as c from tpch_small_exprs o group by category),\n",
    "                            cexps_tpcds as (select category, count(*)::double / (select count(*) from tpcds_small)::double as c from tpcds_small_exprs o group by category)\n",
    "                    select exps.category, \n",
    "                        (select c from cexps c where c.category = exps.category and c.complexity = 'low') as low,\n",
    "                        (select c from cexps c where c.category = exps.category and c.complexity = 'medium') as medium,\n",
    "                        (select c from cexps c where c.category = exps.category and c.complexity = 'high') as high,\n",
    "                        (select c from cexps_tpch c where c.category = exps.category) as tpch,\n",
    "                        (select c from cexps_tpcds c where c.category = exps.category) as tpcds\n",
    "                    from exps\n",
    "                    order by low desc nulls last, medium desc nulls last, high desc nulls last\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "for r in result.fetchall():\n",
    "    def conv(c):\n",
    "        return f\"0\" if c is None else (f\"{c:.3f}\" if c < 0.01 else f\"{c:.2f}\")\n",
    "    print(f\"{r[0]} & {conv(r[1])} & {conv(r[2])} & {conv(r[3])} & {conv(r[4])} & {conv(r[5])} \\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
